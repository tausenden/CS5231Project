{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f2fd815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "CSV_PATH = \"./cleaned_sample_parsed_cadets_tagged_chunked.csv\"\n",
    "MAX_LEN = 1024\n",
    "BATCH_SIZE = 64\n",
    "D_MODEL = 64\n",
    "NHEAD = 4\n",
    "NUM_LAYERS = 2\n",
    "FFN_DIM = 128\n",
    "DROPOUT = 0.3\n",
    "\n",
    "LR = 1e-4\n",
    "EPOCHS = 10\n",
    "SEED = 5231\n",
    "\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d9b4aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df[[\"subject_uuid\", \"sequence\", \"label\"]].dropna()\n",
    "\n",
    "    label2id = {\"normal\": 0, \"attack\": 1}\n",
    "    df = df[df[\"label\"].isin(label2id.keys())].copy()\n",
    "    df[\"y\"] = df[\"label\"].map(label2id)\n",
    "    return df\n",
    "\n",
    "def build_vocab(df):\n",
    "    counter = Counter()\n",
    "    for seq in df[\"sequence\"]:\n",
    "        counter.update(str(seq).split())\n",
    "\n",
    "    token2id = {PAD_TOKEN: 0, UNK_TOKEN: 1}\n",
    "    for i, tok in enumerate(counter.keys(), start=2):\n",
    "        token2id[tok] = i\n",
    "\n",
    "    id2token = {v: k for k, v in token2id.items()}\n",
    "    pad_id = token2id[PAD_TOKEN]\n",
    "    unk_id = token2id[UNK_TOKEN]\n",
    "    vocab_size = len(token2id)\n",
    "    return token2id, id2token, pad_id, unk_id, vocab_size\n",
    "\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, df, token2id, max_len):\n",
    "        self.max_len = max_len\n",
    "        self.token2id = token2id\n",
    "        self.pad_id = token2id[PAD_TOKEN]\n",
    "\n",
    "        self.seqs = []\n",
    "        self.labels = []\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            tokens = str(row[\"sequence\"]).split()\n",
    "            ids = [self.token2id.get(t, self.token2id[UNK_TOKEN]) for t in tokens]\n",
    "            self.seqs.append(ids)\n",
    "            self.labels.append(int(row[\"y\"]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seqs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.seqs[idx], self.labels[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    sequences, labels = zip(*batch)\n",
    "    lengths = [len(s) for s in sequences]\n",
    "    max_len = min(max(lengths), MAX_LEN)\n",
    "\n",
    "    batch_size = len(sequences)\n",
    "    input_ids = torch.full((batch_size, max_len), pad_id, dtype=torch.long)\n",
    "    attention_mask = torch.zeros((batch_size, max_len), dtype=torch.long)\n",
    "\n",
    "    for i, seq in enumerate(sequences):\n",
    "        seq = seq[:max_len]\n",
    "        input_ids[i, :len(seq)] = torch.tensor(seq, dtype=torch.long)\n",
    "        attention_mask[i, :len(seq)] = 1\n",
    "\n",
    "    labels = torch.tensor(labels, dtype=torch.float32)\n",
    "    return input_ids, attention_mask, labels\n",
    "\n",
    "def evaluate(model, data_loader, loss_fn):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, labels in data_loader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            preds = (torch.sigmoid(logits) > 0.5).long()\n",
    "            total_correct += (preds == labels.long()).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "    avg_loss = total_loss / max(1, total_samples)\n",
    "    acc = total_correct / max(1, total_samples)\n",
    "    return avg_loss, acc\n",
    "\n",
    "def evaluate_metrics(model, data_loader, loss_fn):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    y_true = []\n",
    "    y_prob = []\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, labels in data_loader:\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            probs = torch.sigmoid(logits)\n",
    "            loss = loss_fn(logits, labels)\n",
    "\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            y_true.extend(labels.long().tolist())\n",
    "            y_prob.extend(probs.detach().cpu().tolist())\n",
    "    import numpy as np\n",
    "    y_true = np.array(y_true)\n",
    "    y_prob = np.array(y_prob)\n",
    "    y_pred = (y_prob > 0.5).astype(int)\n",
    "    avg_loss = total_loss / max(1, len(y_true))\n",
    "    acc = (y_pred == y_true).mean() if len(y_true) > 0 else 0.0\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n",
    "    try:\n",
    "        auc = roc_auc_score(y_true, y_prob)\n",
    "    except Exception:\n",
    "        auc = float('nan')\n",
    "    return {\n",
    "        \"loss\": float(avg_loss),\n",
    "        \"acc\": float(acc),\n",
    "        \"precision\": float(precision),\n",
    "        \"recall\": float(recall),\n",
    "        \"f1\": float(f1),\n",
    "        \"auc\": float(auc),\n",
    "    }\n",
    "\n",
    "def predict_sequence(model, seq_str, token2id, pad_id, max_len, device):\n",
    "    tokens = str(seq_str).split()\n",
    "    ids = [token2id.get(t, token2id[UNK_TOKEN]) for t in tokens]\n",
    "    ids = ids[:max_len]\n",
    "\n",
    "    input_ids = torch.full((1, max_len), pad_id, dtype=torch.long)\n",
    "    attention_mask = torch.zeros((1, max_len), dtype=torch.long)\n",
    "    input_ids[0, :len(ids)] = torch.tensor(ids, dtype=torch.long)\n",
    "    attention_mask[0, :len(ids)] = 1\n",
    "\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        prob_attack = torch.sigmoid(logits)[0].item()\n",
    "    return prob_attack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2c2b239",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_layers,\n",
    "                 dim_feedforward, dropout, max_len, pad_id):\n",
    "        super().__init__()\n",
    "        self.pad_id = pad_id\n",
    "        self.max_len = max_len\n",
    "\n",
    "        # Token + positional embeddings to encode discrete events with order\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n",
    "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
    "\n",
    "        # Transformer encoder stack for contextual sequence modeling\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=False,\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(d_model, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # input_ids: (B, L), attention_mask: (B, L)\n",
    "        B, L = input_ids.size()\n",
    "        positions = torch.arange(L, device=input_ids.device).unsqueeze(0).expand(B, L)\n",
    "\n",
    "        # Embed tokens + positions, switch to (L, B, D) for torch Transformer\n",
    "        x = self.token_emb(input_ids) + self.pos_emb(positions)\n",
    "        x = x.transpose(0, 1)\n",
    "\n",
    "        # True where token is padding so encoder can ignore it\n",
    "        src_key_padding_mask = (attention_mask == 0)\n",
    "\n",
    "        encoded = self.encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
    "        encoded = encoded.transpose(0, 1)  # back to (B, L, D)\n",
    "\n",
    "        # Masked mean pooling keeps only real tokens when averaging\n",
    "        mask = attention_mask.unsqueeze(-1)\n",
    "        masked_encoded = encoded * mask\n",
    "        summed = masked_encoded.sum(dim=1)\n",
    "        lengths = mask.sum(dim=1).clamp(min=1)\n",
    "        pooled = summed / lengths\n",
    "\n",
    "        pooled = self.dropout(pooled)\n",
    "        logits = self.fc(pooled).squeeze(-1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a26ce506",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    \"\"\"Embedding + LSTM encoder with masked mean pooling for classification.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, d_model, hidden_size, num_layers,\n",
    "                 dropout, pad_id):\n",
    "        super().__init__()\n",
    "        self.pad_id = pad_id\n",
    "\n",
    "        # Token embedding shared with Transformer for fair comparison\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model, padding_idx=pad_id)\n",
    "\n",
    "        # Unidirectional LSTM to capture sequential dependencies\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=d_model,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=False,\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "        if self.fc.bias is not None:\n",
    "            nn.init.zeros_(self.fc.bias)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"Return logits of shape (B,) for the provided batch inputs.\"\"\"\n",
    "        x = self.token_emb(input_ids)  # (B, L, D)\n",
    "\n",
    "        outputs, (h_n, c_n) = self.lstm(x)      # outputs: (B, L, H)\n",
    "\n",
    "        # Masked mean pooling to ignore padded tokens\n",
    "        mask = attention_mask.unsqueeze(-1)  # (B, L, 1)\n",
    "        masked_outputs = outputs * mask\n",
    "        summed = masked_outputs.sum(dim=1)  # (B, H)\n",
    "        lengths = mask.sum(dim=1).clamp(min=1)  # (B, 1)\n",
    "        pooled = summed / lengths  # (B, H)\n",
    "\n",
    "        pooled = self.dropout(pooled)\n",
    "        logits = self.fc(pooled).squeeze(-1)    # (B,)\n",
    "        # logits = self.fc(h_n[-1]).squeeze(-1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a961341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sequence Length Statistics:\n",
      "count    68729.000000\n",
      "mean       218.757366\n",
      "std        672.485184\n",
      "min          3.000000\n",
      "25%         22.000000\n",
      "50%         29.000000\n",
      "75%        124.000000\n",
      "max       5000.000000\n",
      "Name: sequence, dtype: float64\n",
      "90th percentile: 632.0\n",
      "95th percentile: 661.0\n",
      "99th percentile: 5000.0\n",
      "Max length setting: 1024\n",
      "Sequences longer than MAX_LEN: 1862 (2.71%)\n",
      "68729 y\n",
      "0    66617\n",
      "1     2112\n",
      "Name: count, dtype: int64\n",
      "Vocabulary size: 56\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Data loading and split\n",
    "df = load_df(CSV_PATH)\n",
    "token2id, id2token, pad_id, unk_id, vocab_size = build_vocab(df)\n",
    "# Sequence length statistics\n",
    "\n",
    "seq_lengths = df[\"sequence\"].astype(str).apply(lambda x: len(x.split()))\n",
    "print(f\"\\nSequence Length Statistics:\")\n",
    "print(seq_lengths.describe())\n",
    "print(f\"90th percentile: {seq_lengths.quantile(0.9)}\")\n",
    "print(f\"95th percentile: {seq_lengths.quantile(0.95)}\")\n",
    "print(f\"99th percentile: {seq_lengths.quantile(0.99)}\")\n",
    "print(f\"Max length setting: {MAX_LEN}\")\n",
    "print(f\"Sequences longer than MAX_LEN: {(seq_lengths > MAX_LEN).sum()} ({(seq_lengths > MAX_LEN).mean()*100:.2f}%)\")\n",
    "\n",
    "train_df, temp_df = train_test_split(\n",
    "    df, test_size=0.2, stratify=df[\"y\"], random_state=SEED\n",
    ")\n",
    "valid_df, test_df = train_test_split(\n",
    "    temp_df, test_size=0.5, stratify=temp_df[\"y\"], random_state=SEED\n",
    ")\n",
    "\n",
    "train_counts = train_df[\"y\"].value_counts().to_dict()\n",
    "valid_counts = valid_df[\"y\"].value_counts().to_dict()\n",
    "test_counts = test_df[\"y\"].value_counts().to_dict()\n",
    "num_pos = float(train_counts.get(1, 0))\n",
    "num_neg = float(train_counts.get(0, 0))\n",
    "POS_WEIGHT = num_neg / max(1.0, num_pos)\n",
    "\n",
    "sample_weight_values = train_df[\"y\"].map(lambda y: 1.0 / train_counts[y]).values\n",
    "sample_weights = torch.DoubleTensor(sample_weight_values)\n",
    "sampler = torch.utils.data.WeightedRandomSampler(\n",
    "    weights=sample_weights,\n",
    "    num_samples=len(sample_weights),\n",
    "    replacement=True,\n",
    ")\n",
    "\n",
    "train_dataset = SeqDataset(train_df, token2id, MAX_LEN)\n",
    "valid_dataset = SeqDataset(valid_df, token2id, MAX_LEN)\n",
    "test_dataset = SeqDataset(test_df, token2id, MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=sampler, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(len(df), df[\"y\"].value_counts())\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f019c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training Configuration:\n",
      "============================================================\n",
      "Device: cuda\n",
      "Batch Size: 64\n",
      "Max Sequence Length: 1024\n",
      "Dataset Size (total): 68729\n",
      " - Train: 54983  (batches: 860)\n",
      " - Valid: 6873  (batches: 108)\n",
      " - Test : 6873   (batches: 108)\n",
      "Vocabulary Size: 56\n",
      "Class distribution (train): {0: 53293, 1: 1690}\n",
      "Class distribution (valid): {0: 6662, 1: 211}\n",
      "Class distribution (test) : {0: 6662, 1: 211}\n",
      "Loss pos_weight: 31.53\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training configuration display\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Training Configuration:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Max Sequence Length: {MAX_LEN}\")\n",
    "print(f\"Dataset Size (total): {len(df)}\")\n",
    "print(f\" - Train: {len(train_dataset)}  (batches: {len(train_loader)})\")\n",
    "print(f\" - Valid: {len(valid_dataset)}  (batches: {len(valid_loader)})\")\n",
    "print(f\" - Test : {len(test_dataset)}   (batches: {len(test_loader)})\")\n",
    "print(f\"Vocabulary Size: {vocab_size}\")\n",
    "print(f\"Class distribution (train): {train_counts}\")\n",
    "print(f\"Class distribution (valid): {valid_counts}\")\n",
    "print(f\"Class distribution (test) : {test_counts}\")\n",
    "print(f\"Loss pos_weight: {POS_WEIGHT:.2f}\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ea584cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_warmup_steps: 430\n",
      "num_training_steps: 8600\n",
      "\n",
      "============================================================\n",
      "Epoch 1/10\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [02:52<00:00,  4.97batch/s, loss=2.2494, acc=0.5005]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 Summary: TrainLoss=2.2494, TrainAcc=0.5005, ValLoss=2.8562, ValAcc=0.0607, Time=177.5s\n",
      "\n",
      "============================================================\n",
      "Epoch 2/10\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [02:48<00:00,  5.10batch/s, loss=1.7760, acc=0.5423]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 2 Summary: TrainLoss=1.7760, TrainAcc=0.5423, ValLoss=2.7682, ValAcc=0.1803, Time=173.3s\n",
      "\n",
      "============================================================\n",
      "Epoch 3/10\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [02:49<00:00,  5.07batch/s, loss=1.5926, acc=0.5975]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 3 Summary: TrainLoss=1.5926, TrainAcc=0.5975, ValLoss=2.7493, ValAcc=0.2636, Time=174.3s\n",
      "\n",
      "============================================================\n",
      "Epoch 4/10\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [02:48<00:00,  5.09batch/s, loss=1.5297, acc=0.6189]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 4 Summary: TrainLoss=1.5297, TrainAcc=0.6189, ValLoss=2.5596, ValAcc=0.2760, Time=173.6s\n",
      "\n",
      "============================================================\n",
      "Epoch 5/10\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [02:48<00:00,  5.11batch/s, loss=1.4814, acc=0.6317]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 5 Summary: TrainLoss=1.4814, TrainAcc=0.6317, ValLoss=2.6413, ValAcc=0.3087, Time=172.9s\n",
      "\n",
      "============================================================\n",
      "Epoch 6/10\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [02:48<00:00,  5.10batch/s, loss=1.4310, acc=0.6437]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 6 Summary: TrainLoss=1.4310, TrainAcc=0.6437, ValLoss=2.9518, ValAcc=0.3246, Time=173.3s\n",
      "\n",
      "============================================================\n",
      "Epoch 7/10\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [02:51<00:00,  5.02batch/s, loss=1.3233, acc=0.6723]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 7 Summary: TrainLoss=1.3233, TrainAcc=0.6723, ValLoss=3.1747, ValAcc=0.3706, Time=175.8s\n",
      "\n",
      "============================================================\n",
      "Epoch 8/10\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [02:48<00:00,  5.11batch/s, loss=1.2482, acc=0.6908]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 8 Summary: TrainLoss=1.2482, TrainAcc=0.6908, ValLoss=3.5443, ValAcc=0.4067, Time=172.9s\n",
      "\n",
      "============================================================\n",
      "Epoch 9/10\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [02:44<00:00,  5.23batch/s, loss=1.1833, acc=0.7076]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 9 Summary: TrainLoss=1.1833, TrainAcc=0.7076, ValLoss=3.6107, ValAcc=0.4230, Time=169.0s\n",
      "\n",
      "============================================================\n",
      "Epoch 10/10\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 860/860 [02:51<00:00,  5.02batch/s, loss=1.1494, acc=0.7169]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 10 Summary: TrainLoss=1.1494, TrainAcc=0.7169, ValLoss=3.6568, ValAcc=0.4340, Time=175.7s\n"
     ]
    }
   ],
   "source": [
    "from transformers import get_cosine_schedule_with_warmup\n",
    "import time\n",
    "\n",
    "model = TransformerClassifier(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=D_MODEL,\n",
    "    nhead=NHEAD,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dim_feedforward=FFN_DIM,\n",
    "    dropout=DROPOUT,\n",
    "    max_len=MAX_LEN,\n",
    "    pad_id=pad_id,\n",
    ").to(device)\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "total_epochs = EPOCHS\n",
    "num_training_steps = len(train_loader) * total_epochs\n",
    "num_warmup_steps = int(0.05 * num_training_steps)\n",
    "\n",
    "print(f\"num_warmup_steps: {num_warmup_steps}\")\n",
    "print(f\"num_training_steps: {num_training_steps}\")\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Epoch {epoch}/{EPOCHS}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    epoch_start = time.time()\n",
    "\n",
    "    # Add progress bar for batches\n",
    "    pbar = tqdm(train_loader, desc=f\"Training\", unit=\"batch\")\n",
    "    for input_ids, attention_mask, labels in pbar:\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        with torch.no_grad():\n",
    "            preds = (torch.sigmoid(logits) > 0.5).long()\n",
    "            total_correct += (preds == labels.long()).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "        \n",
    "        # Update progress bar with current metrics\n",
    "        current_loss = total_loss / max(1, total_samples)\n",
    "        current_acc = total_correct / max(1, total_samples)\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{current_loss:.4f}',\n",
    "            'acc': f'{current_acc:.4f}'\n",
    "        })\n",
    "\n",
    "    train_loss = total_loss / max(1, total_samples)\n",
    "    train_acc = total_correct / max(1, total_samples)\n",
    "\n",
    "    # Validation at end of epoch\n",
    "    val_loss, val_acc = evaluate(model, valid_loader, loss_fn)\n",
    "\n",
    "    epoch_time = time.time() - epoch_start\n",
    "    print(f\"\\n Epoch {epoch} Summary: TrainLoss={train_loss:.4f}, TrainAcc={train_acc:.4f}, ValLoss={val_loss:.4f}, ValAcc={val_acc:.4f}, Time={epoch_time:.1f}s\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97df17b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training LSTM classifier\n",
      "============================================================\n",
      "\n",
      "------------------------------------------------------------\n",
      "LSTM Epoch 1/10\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LSTM Training: 100%|██████████| 860/860 [00:10<00:00, 81.97batch/s, loss=2.0727, acc=0.4977]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LSTM Epoch 1 Summary: TrainLoss=2.0727, TrainAcc=0.4977, ValLoss=3.0677, ValAcc=0.0326\n",
      "\n",
      "------------------------------------------------------------\n",
      "LSTM Epoch 2/10\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LSTM Training: 100%|██████████| 860/860 [00:10<00:00, 81.59batch/s, loss=1.9922, acc=0.5046]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LSTM Epoch 2 Summary: TrainLoss=1.9922, TrainAcc=0.5046, ValLoss=3.1077, ValAcc=0.0397\n",
      "\n",
      "------------------------------------------------------------\n",
      "LSTM Epoch 3/10\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LSTM Training: 100%|██████████| 860/860 [00:10<00:00, 81.55batch/s, loss=1.9607, acc=0.5062]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LSTM Epoch 3 Summary: TrainLoss=1.9607, TrainAcc=0.5062, ValLoss=2.8694, ValAcc=0.0393\n",
      "\n",
      "------------------------------------------------------------\n",
      "LSTM Epoch 4/10\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LSTM Training: 100%|██████████| 860/860 [00:10<00:00, 80.21batch/s, loss=1.9091, acc=0.5065]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LSTM Epoch 4 Summary: TrainLoss=1.9091, TrainAcc=0.5065, ValLoss=2.8111, ValAcc=0.0332\n",
      "\n",
      "------------------------------------------------------------\n",
      "LSTM Epoch 5/10\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LSTM Training: 100%|██████████| 860/860 [00:10<00:00, 80.29batch/s, loss=1.9087, acc=0.5059]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LSTM Epoch 5 Summary: TrainLoss=1.9087, TrainAcc=0.5059, ValLoss=2.7949, ValAcc=0.0581\n",
      "\n",
      "------------------------------------------------------------\n",
      "LSTM Epoch 6/10\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LSTM Training: 100%|██████████| 860/860 [00:10<00:00, 80.02batch/s, loss=1.7857, acc=0.5271]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LSTM Epoch 6 Summary: TrainLoss=1.7857, TrainAcc=0.5271, ValLoss=2.6599, ValAcc=0.1096\n",
      "\n",
      "------------------------------------------------------------\n",
      "LSTM Epoch 7/10\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LSTM Training: 100%|██████████| 860/860 [00:10<00:00, 82.09batch/s, loss=1.7270, acc=0.5427]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LSTM Epoch 7 Summary: TrainLoss=1.7270, TrainAcc=0.5427, ValLoss=2.2894, ValAcc=0.1689\n",
      "\n",
      "------------------------------------------------------------\n",
      "LSTM Epoch 8/10\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LSTM Training: 100%|██████████| 860/860 [00:10<00:00, 82.83batch/s, loss=1.6716, acc=0.5636]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LSTM Epoch 8 Summary: TrainLoss=1.6716, TrainAcc=0.5636, ValLoss=2.6468, ValAcc=0.1490\n",
      "\n",
      "------------------------------------------------------------\n",
      "LSTM Epoch 9/10\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LSTM Training: 100%|██████████| 860/860 [00:09<00:00, 86.14batch/s, loss=1.6047, acc=0.5790]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LSTM Epoch 9 Summary: TrainLoss=1.6047, TrainAcc=0.5790, ValLoss=2.5389, ValAcc=0.2060\n",
      "\n",
      "------------------------------------------------------------\n",
      "LSTM Epoch 10/10\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LSTM Training: 100%|██████████| 860/860 [00:10<00:00, 85.96batch/s, loss=1.5275, acc=0.5996]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LSTM Epoch 10 Summary: TrainLoss=1.5275, TrainAcc=0.5996, ValLoss=2.6753, ValAcc=0.2328\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Training LSTM classifier\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "LSTM_NUM_LAYERS = 1\n",
    "LSTM_HIDDEN_SIZE = D_MODEL\n",
    "\n",
    "lstm_model = LSTMClassifier(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=D_MODEL,\n",
    "    hidden_size=LSTM_HIDDEN_SIZE,\n",
    "    num_layers=LSTM_NUM_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    "    pad_id=pad_id,\n",
    ").to(device)\n",
    "\n",
    "lstm_loss_fn = nn.BCEWithLogitsLoss()\n",
    "lstm_optimizer = torch.optim.Adam(lstm_model.parameters(), lr=LR)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f\"\\n{'-'*60}\")\n",
    "    print(f\"LSTM Epoch {epoch}/{EPOCHS}\")\n",
    "    print(f\"{'-'*60}\")\n",
    "    lstm_model.train()\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=\"LSTM Training\", unit=\"batch\")\n",
    "    for input_ids, attention_mask, labels in pbar:\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        lstm_optimizer.zero_grad()\n",
    "        logits = lstm_model(input_ids, attention_mask)\n",
    "        loss = lstm_loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "        lstm_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        with torch.no_grad():\n",
    "            preds = (torch.sigmoid(logits) > 0.5).long()\n",
    "            total_correct += (preds == labels.long()).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "        current_loss = total_loss / max(1, total_samples)\n",
    "        current_acc = total_correct / max(1, total_samples)\n",
    "        pbar.set_postfix({'loss': f'{current_loss:.4f}', 'acc': f'{current_acc:.4f}'})\n",
    "\n",
    "    train_loss = total_loss / max(1, total_samples)\n",
    "    train_acc = total_correct / max(1, total_samples)\n",
    "    val_loss, val_acc = evaluate(lstm_model, valid_loader, lstm_loss_fn)\n",
    "    print(f\" LSTM Epoch {epoch} Summary: TrainLoss={train_loss:.4f}, TrainAcc={train_acc:.4f}, ValLoss={val_loss:.4f}, ValAcc={val_acc:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd484e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Final Evaluation\n",
      "============================================================\n",
      "Transformer Test Metrics:\n",
      " - loss: 3.9294\n",
      " - acc: 0.4391\n",
      " - precision: 0.0373\n",
      " - recall: 0.6967\n",
      " - f1: 0.0709\n",
      " - auc: 0.6785\n",
      "\n",
      "LSTM Test Metrics:\n",
      " - loss: 2.5378\n",
      " - acc: 0.2376\n",
      " - precision: 0.0344\n",
      " - recall: 0.8815\n",
      " - f1: 0.0663\n",
      " - auc: 0.6803\n",
      "\n",
      "Transformer inference (normal, attack):\n",
      "2.838618762268652e-08\n",
      "0.047563761472702026\n",
      "\n",
      "LSTM inference (normal, attack):\n",
      "0.019240975379943848\n",
      "0.0042176139540970325\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Final Evaluation\")\n",
    "print(f\"{'='*60}\")\n",
    "transformer_test_metrics = evaluate_metrics(model, test_loader, loss_fn)\n",
    "lstm_test_metrics = evaluate_metrics(lstm_model, test_loader, lstm_loss_fn)\n",
    "\n",
    "print(\"Transformer Test Metrics:\")\n",
    "for k, v in transformer_test_metrics.items():\n",
    "    print(f\" - {k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\nLSTM Test Metrics:\")\n",
    "for k, v in lstm_test_metrics.items():\n",
    "    print(f\" - {k}: {v:.4f}\")\n",
    "\n",
    "example_seq_normal = df[df[\"label\"] == \"normal\"].iloc[0][\"sequence\"]\n",
    "example_seq_attack = df[df[\"label\"] == \"attack\"].iloc[0][\"sequence\"]\n",
    "\n",
    "def get_label(prob):\n",
    "    return 'attack' if prob > 0.5 else 'normal'\n",
    "\n",
    "print(\"\\nTransformer inference (normal, attack):\")\n",
    "p_normal = predict_sequence(model, example_seq_normal, token2id, pad_id, MAX_LEN, device)\n",
    "p_attack = predict_sequence(model, example_seq_attack, token2id, pad_id, MAX_LEN, device)\n",
    "print(f\"Normal sample -> Prob: {p_normal:.4f}, Prediction: {get_label(p_normal)}\")\n",
    "print(f\"Attack sample -> Prob: {p_attack:.4f}, Prediction: {get_label(p_attack)}\")\n",
    "\n",
    "print(\"\\nLSTM inference (normal, attack):\")\n",
    "p_normal_lstm = predict_sequence(lstm_model, example_seq_normal, token2id, pad_id, MAX_LEN, device)\n",
    "p_attack_lstm = predict_sequence(lstm_model, example_seq_attack, token2id, pad_id, MAX_LEN, device)\n",
    "print(f\"Normal sample -> Prob: {p_normal_lstm:.4f}, Prediction: {get_label(p_normal_lstm)}\")\n",
    "print(f\"Attack sample -> Prob: {p_attack_lstm:.4f}, Prediction: {get_label(p_attack_lstm)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
